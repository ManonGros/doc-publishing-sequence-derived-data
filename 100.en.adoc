== Introduction 

=== Rationale

The last 20 years have brought an increased understanding of the immense power of molecular methods for documenting the diversity of life on earth. Seemingly sterile and mundane substrates such as soil and sea water turn out to abound with life – although perhaps not in a way that the casual observer may immediately appreciate. Nevertheless, it is there. DNA-based studies have shown that organism groups such as fungi, insects, oomycetes, bacteria and archaea are everywhere, although we often cannot observe them physically ([Debroas et al. 2017]). And it’s not just in the microscopic world, there are many species that are at least theoretically possible to observe physically but it is potentially very costly, labour-intensive, and perhaps invasive to seek to do so, such as some fish species ([Boussarie et al. 2018]). In all these situations, DNA sequence data let us record the presence (and past presence) of these organisms non-invasively and with minimal effort. With this in mind, it is hard to argue that we need tangible, physical manifestations of all organisms present at some site in order to record them. Neither would we wish to make the assumption that only physically observable organisms are important when it comes to biodiversity, ecology, and biological conservation.

So what should we do with these inconspicuous or otherwise unobservable taxa, since they often fall below the radar of vetted protocols for field work, checklists, depositions into natural science collections, and so on? The present document argues that we must record and report on them in perhaps the only way we can right now: through DNA data. In fact, the current maturity of DNA methodologies puts us in a position to record the presence of these organisms to a level of detail that exceeds that of macroscopic observations of organisms in general. We are thus at a unique point in history: this is where we define how we would like to record and report on an organism as present in some substrate or locality through molecular data. If this was to be left unspecified, then presumably disorder would follow. After all, there is no shortage of examples of where the lack of standards and recommendations have led to very heterogeneous and largely incomparable bodies of data ([Leebens-Mack et al. 2006]; [Yilmaz et al. 2011]; [Nilsson et al. 2012]).

DNA-derived occurrence data of species should be as standardized and reproducible as possible, regardless of whether the underlying species have formal scientific names or not. In some cases, these occurrence data will hint at previously unknown geographical and ecological properties of described species, thus enriching our body of knowledge on these species. In many other cases, these data will allow us to amalgamate and visualize information on as-yet undescribed species, thus hopefully speeding up their eventual formal description. We are aware that this represents a major addition to the many ways in which GBIF and many other biodiversity databases and resources index our living world, but it is a change we feel that science, policy makers, and other users of biodiversity data are ready – indeed, eager – to embrace. Recent estimates suggest that at least 85% of all extant species are undescribed, so we must not let the remaining <15% dictate the standards by which the majority of extant species should be characterized ([Mora et al. 2011]; [Tedesco et al. 2014]).

This document seeks to lay out the way in which DNA-derived occurrence data should be reported for standardized inclusion into GBIF and other biodiversity resources. There is a range of complications to consider, and we urge users to take the proper time to read up prior to commencing the field work, let alone the sequence generation, processing, and deposition. That said, we do want to see molecular occurrences reported to GBIF and elsewhere, and we hope that the natural science community and users of biodiversity data at large will be quick to embrace broad and open sharing of sequence-derived data as a paradigm shift from option and best practice to a community norm.

Biodiversity discovery platforms are well aware of the complications and polarized opinions arounds digital sequence information in the context of the https://www.cbd.int/doc/c/ba60/7272/3260b5e396821d42bc21035a/dsi-ahteg-2020-01-07-en.pdf[Convention on Biological Diversity (2020)]. According to the classification of information related to a genetic resource in this report, genetic sequence information resurfaced through the biodiversity discovery platforms belongs to Group 1, DNA and RNA (associated data to nucleic acid reads; non-coding nucleic acid sequences; in some cases nucleic acid sequence reads) and also Associated information (Information associated with digital sequence information… for example, biotic and abiotic factors in the environment or associated with the organism; other types of information associated with a genetic resource). Our recommendation is to share and archive genetic sequence information as openly as possible and with as accurate and full metadata as possible through the INDSC. Using this guide, sequence-derived biodiversity data should be resurfaced and actualized through biodiversity discovery platforms under the appropriate Creative Commons designation, such as https://creativecommons.org/publicdomain/zero/1.0/[CC0], https://creativecommons.org/licenses/by/4.0/[CC BY 4.0] or https://creativecommons.org/licenses/by-nc/4.0/[CC BY-NC 4.0].

Even though this guide could be, in principle, used to publish records linking to whole genomes sampled in the environment, this is, in most cases, an unnecessary overkill. The main, if not the only reason, why ASV would be published as an attribute of the sequence-derived occurrence record is to have a reusable basis of name or of OTU. As sequence reference databases develop very rapidly, availability of ASVs enables regular re-identification, quality control and gradual improvement of taxonomic annotations for the existing sequence-derived data: a speed and scale luxury we can only dream of with morphological re-identifications of physical material. Closing this opportunity will have a detrimental effect on data quality and data longevity of the sequence-derived occurrence data. However, this guide leaves data publishers an option to replace ASVs with ASV identifiers.

At the moment of writing, genetic barcodes and metabarcodes are typically short fragments of non-coding or coding genes, which are not suitable for commercial exploitation such as use in big pharma or enzyme development. As archival of sequences through INDSC is a widespread norm in sequence based research, publication of occurrence data originating from sequences does not involve publishing new sequences, as in most, ideally in all cases, they are in a public genetic repository already. Therefore, the added value relates to spatial/temporal occurrences and sequence bases names, and not to the genetic information itself.

There are many reasons why a user would want to report their taxa in open and reproducible ways. Increased citability, highlighting these taxa in the context of biological conservation, and making a contribution towards future taxonomic and ecological progress come to mind, all three of which are likely to be at the heart of research professionals doing environmental sequencing or using such data. This assumes some kind of attribution of scientific credit, which the biodiversity community clearly should work to accomplish. Indeed, with this move we hope to highlight a most important proportion of extant biodiversity and speed up its discovery rate and integration into biological conservation and policy making. This is a time of change, and we should all change with it.

=== Target audiences

These guide were put together with multiple target audiences in mind: students planning a first sequence based study, researchers with old sequences and abundance tables they want to revive, biodiversity portal specialists who are new to sequence-derived data, and bioinformaticians well familiar with sequence data, but new to biodiversity discovery platforms. We’re not directly targeting users of molecular data in biodiversity discovery platforms, but if you are one and read this to get an understanding of what the data represents, you might find chapter 2.7 “Outputs”, particularly interesting. Our intention is to provide you with a guide on what to do genetic sequence data and associated attributes to get them published through general biodiversity discovery platforms.

The flowchart (Fig. 1) outlines the processing steps involved in publishing amplicon-derived molecular biodiversity data in repositories such as GBIF and national biodiversity discovery platforms such as those built on the ALA platform. This guide’s focus is primarily on the steps following the arrival of raw (fastq [have link to description and glossary]) sequences from the sequencing step. Familiarizing yourself with the flowchart, and making a note of which steps appear familiar and which do not, will help you navigate the guide.

[[figure-01]]
image::img/web/figure-01.jpg[]
.Figure 1. Overall workflow for sequence-derived biodiversity data as described in this guide.

We have done our best to phrase this guide so that it is useful for all who feel included in the above described audiences, but background reading (e.g. https://www.gbif.org/publishing-data[GBIF quick guide to data publishing]) can be required in certain cases.

=== Introduction to sequence derived occurrence data

Sequence derived biological occurrence data includes environmental DNA (eDNA) (DNA extracted from environmental samples or, rather than from individual organisms (Thomsen and Willerslev 2015) and, from bulk samples comprising many individuals (e.g., plankton samples or Malaise trap samples consisting of multiple individuals from many species). The most common form of sequence-derived occurrence data is currently that derived from eDNA, and because the analysis methods and end products are very similar between the different sample sources the discussion below will focus on eDNA (sections 3.1.1 and 3.2.1), noting that it is applicable to the other sources.

These methods often utilize targeted sequencing of taxonomically and phylogenetically informative genetic markers, but can also be used in e.g., qPCR-based approaches that do not directly result in DNA sequence data ([sections 3.1.3] and [3.2.3]). Environmental DNA is a relatively new concept as it is currently understood, but the term has been in use since 1987, when used to describe DNA from microbes in sediment samples (. https://doi.org/10.1016/0167-7012(87)90025-x[Ogram, Sayler, and Barkay 1987^]). eDNA is now more broadly used to describe a complex mix of DNA from different organisms ([Taberlet et al. 2018] and [2012]). Thus, eDNA is all DNA extracted from a specific environmental sample, regardless of the substrate of the sample and which species it contains. DNA in the environment can be derived from, e.g., skin and hair cells, saliva, soil, feces, and from living or recently dead organisms (. https://doi.org/10.1007/s00374-008-0345-8[Pietramellara et al. 2009^]). Environmental DNA often sufficiently represents all organisms in a given sample. In practice, however, the presence of DNA in the environmental sample depends on the body size of the organism, morphology, activity level, habitat selection and sampling methods used to capture it ([Taberlet et al. 2018]).

[[figure-02]]
image::img/web/figure-02.jpg[]
.Figure 2.

Several studies show that for water samples, analyses based on eDNA may have a higher probability of finding rare and cryptic species than conventional methods ([Thomsen et al. 2012]; [Biggs et al. 2015]; [Valentini et al. 2016]; [Bessey et al. 2020]). The same may be true for other environmental samples. Therefore, eDNA may be suitable for monitoring rare red list species and undesirable alien species that often have low densities and that are difficult to detect with conventional methods. Environmental DNA methods are able to detect cryptic organisms, especially those that are small and unable to be detected by the naked eye (e.g. bacteria and fungi). In addition, eDNA can also be used for observation of many species simultaneously, and may describe all or components of biological communities (https://ntnuopen.ntnu.no/ntnu-xmlui/handle/11250/2612638[Ekrem & Majaneva 2019^]).

Identification and classification of organisms from sequence data and marker-based surveys is dependent upon a reference library of known organisms to match the newly generated sequences to. The efficacy of classification depends on the completeness (coverage) and the reliability of reference libraries, as well as the tools used to carry out the classification. All of these are moving targets, making taxonomic expertise and caution important qualities in the assessment of the results ([Chapter 2.6]). Availability of ASVs is fundamental for subsequent re-identifications and improvements of identification accuracy.

Some studies show a relationship between the amount of DNA in an environmental sample and the biomass of the species in the environment. One can therefore potentially also think of environmental DNA allowing a so-called semi-quantitative estimate (indirect target) for organism biomass, both from environmental samples and bulk samples ([Takahara et al. 2012]; [Thomsen et al. 2012]; [Andersen et al. 2012]; https://doi.org/10.1038/ismej.2013.61[Ovaskainen et al. 2013^]; Lacoursière-[Roussel, Rosabal, and Bernatchez 2016]); [Thomsen et al. 2016]; [Valentini et al. 2016]; https://doi.org/10.1002/edn3.45[Fossøy et al. 2019^]; https://doi.org/10.1002/edn3.7[Yates, Fraser, and Derry 2019^]; [Doi et al. 2017]). However, other studies show little correlation between environmental DNA quantity and estimated population density ([Knudsen et al. 2019]), and PCR, quantification, mixing and other biases are frequently debated. For example, shell change, reproduction and mass death can contribute to increased levels of crustacean environmental DNA in water, while turbidity and poor water quality reduce the amount of detectable environmental DNA (https://doi.org/10.1111/1365-2664.13404[[Strand et al. 2019^]). Similarly, large multicellular organisms are likely to shed more eDNA than microscopic ones ([Elbrecht & Leese 2015]). Quantitative estimates of population sizes based on environmental DNA will therefore require more testing before it becomes a widespread and accepted method. Without careful calibration, biological observations derived from eDNA should be thought of "observed presences" or perhaps "relative abundances" (where relative refers to among all the observations made from a particular sample and assay), rather than reliably indicating actual abundance or absence.

Environmental DNA is thus a sample type, not a method. The starting point for eDNA investigations, therefore, includes DNA taken from any environmental sample where the DNA of a captured individual is not specifically targeted in the field. This includes water, soil, sediment and air, but also stool samples and tissue (plant/animal) where the host DNA is not targeted ([Taberlet et al. 2018]). Note, however, that choice of PCR primers sets taxonomic limits and introduces biases to signals of community compositions and abundances. To study environmental DNA, there are a number of methods of analysis ([Text Box 3.2.1]). You can divide these into two main types where one wants to 1) detect a specific organism or 2) describe a community of a range of organisms. Different methods of analysis will generate different types and volumes of data. Most often DNA concentrations are low, and technical and biological replicates should be included to validate species detection.

=== Introduction to biodiversity publishing

Publishing biodiversity data is largely a process of making species occurrence data findable, accessible, interoperable and reusable, in accordance with the FAIR principles (https://doi.org/10.1038/sdata.2016.18[Wilkinson et al. 2015^]). Biodiversity data discovery platforms help expose and discover genetic sequence data as biodiversity occurrences alongside other types of biodiversity data, such as museum collections, citizen science data etc. However, the structure, management, and storage of data will vary between different original data sources as a reflection of the local data needs, implying that data discovery, access and reuse requires making individual datasets compatible with each other. This is an important endeavour, as publishing sequence derived data in biodiversity data discovery platforms help address global taxonomic, spatial and other inconsistencies in the current global biodiversity data, by making data available through single access points for large-scale data-intensive research, management, and policy. The compatibility between datasets is reached through the process of standardization.

There are a number of data standards for general biodiversity data, and a separate set of standards for genetic sequence data. Standards often have subsets of the most important or most frequent sets of fields, which are called “cores”. The preferred format for publishing data in the GBIF and ALA networks is the Darwin Core Archive (DwC-A). In practice, this is a compressed folder (a zip file) containing data files, in standard comma- or tab-delimited text format, a metadata file (eml.xml) describing the data resource , and a metafile (meta.xml) specifying the structure of files and data fields included in the archive. Chapter 3 of this guide provides recommendations for the mapping of the data files, while guidelines and tools for constructing the xml files can be found here: https://doi.org/10.1038/sdata.2016.18[Darwin Core^] (DwC) or https://www.tdwg.org/standards[TDWG^], https://www.gbif.org/standards[GBIF^], and https://support.ala.org.au/support/solutions/articles/6000195499-what-are-biodiversity-data-standards-[ALA^].

A central part of standardization is field mapping, which is the transformation of the original field (column) structure in the source data export into a standard field structure. Standardization may also affect the content of the individual fields within each record, for instance recalculation of coordinates to a common system, re-arrangement of the date elements, or the contents of fields may be mapped to a standard set of values, often called a vocabulary. The process of standardization also provides an opportunity to improve data quality: fill in omissions, correcting typos and extra spaces, and handle heterogeneities and outliers, etc. There is no doubt that improving the quality of data will increase its suitability to be reused, but at the same time published data in any state is better than data that is unpublished.

[[figure-03]]
image::img/web/figure-03.jpg[]
.Figure 3.

Standardized biodiversity data and associated metadata are often packaged into particular formats for efficient functioning of the databases and portal. An example of such a package is the Darwin Core Archive (DwC-A) which is detailed in [Chapter 3]. Packaged standardized biodiversity data – in the DwC-A case a compressed archive (a zip file) containing data, metadata and a file describing the structure of the archive – can travel between systems using specific data exchange protocols.

Once a dataset has been through the standardizаtion and data quality processes, the dataset needs to be placed in an online location and be attributed with metadata. Metadata – data or information about the dataset – is the set of key parameters that describe a dataset and further improve its discoverability and reuse. Metadata information contains such important elements as authorship, DOI, organizational affiliations and many other provenance parameters, as well as procedural and methodological information about how the dataset was collected and curated.

Datasets and their associated metadata are indexed by each data portal: this enables the data to be searched, allows queries, filters and other data access features, to be applied through APIs or web portals. Unlike journal publications, datasets are dynamic products, and can have multiple versions, changing number of records, and non-immutable metadata fields under the same title and DOI.

Note that most holders of genetic sequence data are expected to upload and archive genetic sequence data in raw sequence data repositories such as NCBI’s https://www.ncbi.nlm.nih.gov/genbank/submit/[SRA^] or EMBL’s https://biodiversitydata-se.github.io/mol-data/ena-metabar.html[ENA^], a topic that is not covered here. Biodiversity data discovery platforms such as ALA, GBIF, and most national biodiversity portals are not archives or repositories for raw sequence reads and associated files. We do, however, stress the importance of maintaining links between such primary data and derived occurrences in [Chapter 3].

=== Processing workflows – from sample to ingestible data 

Metabarcoding data can be produced with a number of different sequencing platforms (Illumina, PacBio, Oxford Nanopore, Ion Torrent, etc.) that rely on different principles for readout and generation of data that differ with respect to read length, error profile, whether sequences are single or paired-end, etc. Currently the Illumina short read platform is the most developed and as such is the basis of the below descriptions, however, the bioinformatics processing of the data follows the same general principles (QC, denoising, classification) regardless of the sequencing technology used (https://doi.org/10.3389/fmicb.2017.0156[Hugerth et al. 2017^], [Fig 2]).

[[figure-04]]
image::img/web/figure-04.jpg[]
.Figure 4. Outline of bioinformatic processing of metabarcoding data.

Typically, the DNA sequences are first pre-processed by removing primer sequences and, depending on the sequencing method used, low quality bases, usually toward the 5’ and 3’ sequence ends. Sequences not fulfilling requirements on length, overall quality, presence of primers, etc. are typically removed.

The pre-processed sequences can thereafter be classified into taxa using reference databases (closed reference methods), or processed further to taxonomy independent abundances (open reference methods). Open reference methods are perhaps the most common and require either clustering of sequences into operational taxonomic units (OTUs; [Blaxter et al. 2005]) of some defined sequence similarity, or denoising sequences to produce amplicon sequence variants (ASV; also referred to as zero radius OTU (zOTU)). Denoising attempts to correct errors that have been introduced in the PCR and/or sequencing steps, such that the denoised sequences are the set of unique biologically real sequences present in the original sequence mixture. In case of paired-end sequences, the forward and reverse sequences may be denoised separately and merged or merged prior to denoising. The resulting set of amplicon sequence variants (ASVs; https://doi.org/10.1038/ismej.2017.119[Сallahan et al. 2017^]) can differ by as little as one base. Operationally, ASV's may be thought of as OTU's without defined radius and while denoising algorithms are typically very good, they do not entirely remove the problems of over-splitting or lumping sequences. 

The PCR used for generating the sequencing library can result in the generation of artefactual sequences in the form of chimeras; a single sequence that originates from multiple parent sequences. Such sequences can be detected bioinformatically and removed, and this is typically done after OTU clustering or denoising.

Finally, the pre-processed sequences, OTUs or ASVs, are taxonomically classified by comparing them to a database of annotated sequences (often referred to as reference libraries, see Section 2.6 [internal link]). As for the previous steps, a suite of methods are available, but most of them are either based on aligning the metabarcoding sequences to the reference sequences or by counting shared k-mers (short exact sequences) between these.

There are a number of open source pipelines available for bioinformatic processing of metabarcoding data (lQIIME, DADA2, SWARM, USEARCH, mothur, LULU, PROTAX) [LINKS]. Given many popular and well used workflows exist we make some recommendations below for the analysis of data for submission to biodiversity discovery platforms, not to suggest that they are the best methods or most appropriate for all purposes, but in an attempt to encourage submission of relatively standardized data that may be easily comparable to the platforms. If possible, a well documented and maintained workflow should be used (e.g. https://nf-core/ampliseq[nf-core/ampliseq pipeline]). Metadata should include workflow details and versions (SOP in MiXS extension, see mapping 3.3. Sequence data should be deposited in an appropriate nucleotide archive (NCBI’s SRA ([Leinonen et al. 2011]) or EMBL’s ENA ([Amid et al. 2020])) and data submitted to the biodiversity discovery platform using the biosample ID obtained from the archive (see 3.3 data mapping). Making use of these sample ID’s will reduce the chances of duplication and ensure sequence data is readily obtainable should opportunities for re-analysis arise, as reference libraries and bioinformatic tools improve. The core end product of these pipelines is typically a file of counts of individual OTUs or ASVs in the different samples together with the taxonomy assigned to these. This is either output in tabular format or in the BIOM format. Usually OTU or ASV sequences are additionally provided in the FASTA format (Pearson & Lipman 1988).

=== Taxonomy of sequences

Taxonomic annotation of sequences is a critical step in the processing of molecular biodiversity datasets, as scientific names are key to accessing and communicating information about the observed organisms. The accuracy and precision of such sequence annotation will depend on the availability of reliable reference databases/libraries across all branches of the tree of life, which in turn will require joint efforts from taxonomists and molecular ecologists.

Species are described primarily by taxonomists, placing taxonomy at the heart of biology. Any attempt at characterizing biodiversity, like many other scientific and societal efforts, must therefore use the end product of taxonomic research. However, unlike DNA sequence data, taxonomic outputs may not always be readily amenable to direct algorithmic or computational interpretation: classical taxonomy is a human driven process which includes manual steps of taxon delimitation, description and naming, and result in a formal publication in accordance to the international Codes of Nomenclature. Additionally, as illustrated in previous chapters, DNA sequence-based surveys are very good at detecting cryptic species and will often identify the presence of organisms currently outside traditional Linnaean taxonomic knowledge. While this guideline does not seek to cover the publication of alternative species checklists derived from sequence data, this disparity between traditional taxonomy and eDNA efforts is in nobody’s interest, and so we offer the following recommendations to readers of this guide.

Given how critically central taxonomy is to the discovery of biodiversity data, it is highly recommended that any eDNA sequencing efforts always seek to include relevant taxonomic expertise in their study. It would similarly be beneficial if eDNA sequencing studies allocate a small portion of their budget to the generation and release of reference sequences from previously unsequenced type specimens or other important reference material from the local herbarium, museum, or biological collection. Taxonomists, too, could contribute towards this goal by always bundling relevant DNA sequences with each new species description ([Miralles et al. 2020]) and by targeting the many novel biological entities unraveled by eDNA efforts (e.g. [Tedersoo et al. 2017]).

Most current biodiversity discovery platforms are based on traditional name lists and taxonomic indexes. Given that sequence-derived occurrences are rapidly becoming a significant source of biodiversity data, and that official taxonomy and nomenclature for these lag behind, we think data providers and platforms should continue to explore and include more flexible representations of taxonomy, such as molecular reference databases [UNITE and BOLD examples from GBIF] that recognize sequence data as reference material for not previously classified organisms (e.g., GTDB, iBOL BINs, UNITE), into their taxonomic backbones and that other commonly used databases (e.g., PR2, RDP, SILVA) develop stable identifiers for taxa and make reference sequences available for those taxa. In contrast to classical taxonomy, clustering DNA sequences uses similarity and other signals (such as phylogenetic and probability) algorithmically, but may include human editing. The resulting OTUs vary in stability, presence of reference sequences and physical material, alignments and cut-off values, OTU identifiers, such as DOI, and most importantly in scale (local - study or project specific vs. global - enabling cross-study comparisons). Differing from Linnaean taxa, which are formally described in research publications, OTUs live in the evolving digital reference libraries which differ by taxonomic focus, barcode genes and other factors.

Algorithms for taxonomic annotation of eDNA will typically assign each unique sequence to their nearest taxonomic group in a reference set, given some relatedness and confidence criteria. For poorly known groups of organisms, e.g. prokaryotes and fungi, this may be a non-Linnean placeholder name for a (cluster-based) taxon, and this taxon will often be ranked above species level. No reference database contains all species in a given group, unlike what many users seem to think: this misunderstanding is a source of numerous taxonomic misidentifications during the last 30 years.

During import into the biodiversity platform, the taxonomic resolution of these occurrences may be reduced even further, as the reference set used for annotation may not be included in the taxonomic index of that platform. Therefore the inclusion of the underlying sequence in each record will allow future users to potentially identify the organism to a greater level of granularity, particularly as reference libraries improve over time. In cases where the underlying sequence cannot be bundled with the submission, we advocate deposition of a (Latin or placeholder) name of the taxon plus an MD5 checksum of the sequence as a unique taxon ID (see [3.3 Data Mapping]). MD5 checksums are unidirectional hash algorithms commonly used for verifying file integrity and storing passwords (ref), but in this case would produce a unique and repeatable representation of the original sequence that would not allow its regeneration. MD5 checksums enable efficient query of whether that exact sequence has been recovered in other eDNA efforts, but it is not a complete replacement of the sequence as MD5s do not enable further analyses. Two sequences differing by even a single base will get two completely different MD5 checksums, such that BLAST-style sequence similarity searches do not apply.

=== Outputs

The purpose of exposing sequence-derived data through biodiversity platforms is to enable reuse of these data together with other biodiversity data types. It is very important to keep this reuse in mind when preparing your data for publication. Ideally, the metadata and data should tell a complete story in such as way that new, uninformed users would be able to utilize this evidence without any additional consultations or correspondence.Biodiversity discovery platforms provide search, filter, browsing and data access functionality [link to new GBIF data use webpage]. Users can often choose data outputs (e.g. DwC-A, CSV) and then process, clean, and transform data into the shape and format needed for the analyses.

At GBIF.org or through GBIF API, registered users can search, filter and download & access biodiversity data using one of the three output options. Simple: a simple, tab-delimited format which includes only the GBIF-interpreted version of the data, as a result of the indexing process. Good for making quick tests and importing directly to spreadsheets. Darwin Core Archive: richer format that includes all data - interpreted as well as the original verbatim version provided by the publisher (prior to indexing and interpretation by GBIF). It includes all the metadata and issue flags provides a richer view of the downloaded dataset. Species List: a simple table format that includes an interpreted list of unique species names from a dataset. Together with the link to download that data query, each GBIF user receives data citation with a unique DOI. DOI-based citation of data queries provides recognition and credit to data originators, and improves credibility and transparency of findings based on these data.

UNITE is a web-based sequence management environment centred on the eukaryotic nuclear ribosomal ITS region. All public such sequences are clustered into species hypotheses (SHs), which are assigned unique DOIs. An SH matching service outputs information on, e.g., what species are present in eDNA samples, whether these species are potentially undescribed new species, in what other studies they were recovered, whether the species are alien to a region, or whether they are threatened. The DOIs are connected to the taxonomic backbone of PlutoF and https://www.gbif.org[GBIF], such that they are accompanied by a taxon name where available.

The data used in UNITE are hosted and managed by the https://plutof.ut.ee[PlutoF platform]. Data are represented through a range of standards, primarily https://dwc.tdwg.org/[Darwin Core], https://gensc.org/mixs/[MIxS], and https://github.com/RDA-DMP-Common/RDA-DMP-Common-Standard[DMP Common Standard]; partial support is available for https://www.dcc.ac.uk/resources/metadata-standards/eml-ecological-metadata-language[EML], https://pubmed.ncbi.nlm.nih.gov/20211251/[MCL], and https://terms.tdwg.org/wiki/GGBN_Data_Standard[GGBN].

PlutoF exports data primarily through the CSV and FASTA formats. PlutoF can also be used to publish data in GBIF (using the DwC format) and to prepare GenBank submission files. It is furthermore possible to download species lists from your data and download your project as a https://www.json.org/json-en.html[JSON] document with project data in hierarchically structured.

It is essential to follow data citation recommendations and use DOIs, as good data citation culture is not only the academic norm, but also a powerful mechanism to credit and acknowledge, and therefore incentivize data publishers.
